"""
Streamlit å‰ç«¯ - æ®µè½çº§åˆ« AIGC æ£€æµ‹å·¥å…·
æ”¯æŒé€æ®µè½æ£€æµ‹å’Œå¯è§†åŒ–æ˜¾ç¤º
"""

import streamlit as st
import pandas as pd
from advanced_detector import ChineseAIGCDetector
import plotly.graph_objects as go
from typing import List, Dict, Tuple
import re
import html
import PyPDF2
from docx import Document
import io
import jieba
import numpy as np
from collections import defaultdict

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="AIGC æ£€æµ‹å™¨",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è‡ªå®šä¹‰ CSS
st.markdown("""
    <style>
    /* ç´§å‡‘å¼æ®µè½æ ·å¼ - ç±»ä¼¼åŸæ–‡å¸ƒå±€ */
    .compact-text-container {
        font-size: 16px;
        line-height: 1.8;
        margin: 20px 0;
        padding: 0;
    }
    .text-segment {
        display: inline;
        padding: 2px 0;
        line-height: 1.8;
        transition: all 0.2s ease;
    }
    .text-segment:hover {
        opacity: 0.85;
        cursor: pointer;
    }
    .char-segment {
        display: inline;
        padding: 2px 0;
        line-height: 1.8;
        transition: all 0.2s ease;
        position: relative;
    }
    .char-segment:hover {
        opacity: 0.85;
        cursor: pointer;
    }
    /* AI ç‡é«˜ - çº¢è‰²èƒŒæ™¯ */
    .highlight-high {
        background-color: rgba(255, 100, 100, 0.25);
        border-bottom: 2px solid #ff6464;
    }
    /* AI ç‡ä¸­ - é»„è‰²èƒŒæ™¯ */
    .highlight-medium {
        background-color: rgba(255, 193, 7, 0.25);
        border-bottom: 2px solid #ffc107;
    }
    /* AI ç‡ä½ - ç»¿è‰²èƒŒæ™¯ */
    .highlight-low {
        background-color: rgba(76, 175, 80, 0.2);
        border-bottom: 2px solid #4caf50;
    }
    /* å†…è”æ ‡è®° */
    .inline-badge {
        display: inline-block;
        font-size: 11px;
        padding: 2px 6px;
        margin: 0 3px;
        border-radius: 3px;
        font-weight: bold;
        vertical-align: super;
        line-height: 1;
    }
    .inline-badge-high {
        background-color: #ff6464;
        color: white;
    }
    .inline-badge-medium {
        background-color: #ffc107;
        color: #333;
    }
    .inline-badge-low {
        background-color: #4caf50;
        color: white;
    }
    /* å›¾ä¾‹ */
    .legend-container {
        display: flex;
        gap: 20px;
        padding: 15px;
        background-color: #f8f9fa;
        border-radius: 8px;
        margin: 15px 0;
        font-size: 14px;
    }
    .legend-item {
        display: flex;
        align-items: center;
        gap: 8px;
    }
    .legend-color {
        width: 40px;
        height: 20px;
        border-radius: 3px;
        border: 1px solid #ddd;
    }
    /* ç»†å°çš„å†…è”ç¼–è¾‘æŒ‰é’®ï¼šç¬”å›¾æ ‡ï¼Œæå°å°ºå¯¸ */
    .stTooltipHoverTarget button {
        border: none !important;
        background: transparent !important;
        padding: 0 3px !important;
        min-width: 16px !important;
        min-height: 14px !important;
        height: 16px !important;
        font-size: 12px !important;
        line-height: 1 !important;
        box-shadow: none !important;
        margin: 0 !important;
    }
    </style>
""", unsafe_allow_html=True)

# Dialog support check
if hasattr(st, "dialog"):
    dialog_decorator = st.dialog
elif hasattr(st, "experimental_dialog"):
    dialog_decorator = st.experimental_dialog
else:
    dialog_decorator = None

def edit_form_content(index, detector: ChineseAIGCDetector = None):
    """ç¼–è¾‘è¡¨å•å†…å®¹"""
    if "results" not in st.session_state or index >= len(st.session_state.results):
        st.error("æ•°æ®é”™è¯¯")
        return

    item = st.session_state.results[index]
    active_detector = detector or st.session_state.get("detector")
    
    # ä¸´æ—¶ç»“æœ key (ç”¨äºé‡ç®—ä½†ä¸æäº¤çš„æƒ…å†µ)
    temp_result_key = f"temp_result_{index}"
    display_item = st.session_state.get(temp_result_key, item)

    # è‹¥ä¸Šæ¬¡ç‚¹å‡»äº†é‡ç½®ï¼Œæ¸…ç†è¾“å…¥æ¡†çŠ¶æ€ï¼Œå†æ¸²æŸ“æ–°çš„é»˜è®¤å€¼
    reset_flag_key = f"reset_request_{index}"
    if st.session_state.get(reset_flag_key):
        st.session_state.pop(reset_flag_key, None)
        st.session_state.pop(f"edit_area_{index}", None)
        st.session_state.pop(temp_result_key, None) # æ¸…é™¤ä¸´æ—¶ç»“æœ
        display_item = item # å›é€€åˆ°åŸå§‹ç»“æœ
    
    metric_slot = st.empty()
    render_ai_metric(metric_slot, display_item["AIç‡"])
    
    # Original Text (Always Visible)
    st.text_area("åŸæ–‡æ˜¾ç¤º", value=item.get("åŸæ–‡", item["æ–‡æœ¬"]), disabled=True, height=100)
    
    # Contribution View (Inserted between Original and Current if calculated)
    contrib_key = f"contrib_results_{index}"
    if contrib_key in st.session_state:
        html_content = generate_contribution_html(st.session_state[contrib_key])
        st.markdown(html_content, unsafe_allow_html=True)
    
    # Use a key that depends on the index to avoid conflicts, but we need to be careful with state
    # If we use key, streamlit manages the value.
    new_text = st.text_area("å½“å‰å†…å®¹", value=display_item["æ–‡æœ¬"], height=150, key=f"edit_area_{index}")
    
    col1, col2, col3, col4 = st.columns(4)
    if col1.button("æäº¤", type="primary", key=f"submit_{index}", use_container_width=True):
        st.session_state.results[index]["æ–‡æœ¬"] = new_text
        if active_detector:
            # æ£€æŸ¥æ˜¯å¦æœ‰ä¸´æ—¶ç»“æœä¸”æ–‡æœ¬ä¸€è‡´ï¼Œè‹¥æ˜¯åˆ™ç›´æ¥ä½¿ç”¨
            if temp_result_key in st.session_state and st.session_state[temp_result_key]["æ–‡æœ¬"] == new_text:
                recalc = st.session_state[temp_result_key]
                st.session_state.results[index].update({
                    "AIç‡": recalc["AIç‡"],
                    "äººç±»ç‡": recalc["äººç±»ç‡"],
                    "ç½®ä¿¡åº¦": recalc["ç½®ä¿¡åº¦"],
                    "é¢„æµ‹": recalc["é¢„æµ‹"]
                })
            else:
                recalc = active_detector.detect_single(new_text)
                st.session_state.results[index].update({
                    "AIç‡": recalc["ai_prob"],
                    "äººç±»ç‡": recalc["human_prob"],
                    "ç½®ä¿¡åº¦": recalc["confidence"],
                    "é¢„æµ‹": recalc["prediction"]
                })
        
        # æ¸…ç†çŠ¶æ€
        st.session_state.pop(temp_result_key, None)
        st.session_state.dialog_open = False
        st.session_state.editing_index = None
        if contrib_key in st.session_state:
            del st.session_state[contrib_key]
        st.rerun()
        
    if col2.button("é‡ç®— AIç‡", key=f"recalc_{index}", use_container_width=True):
        if active_detector is None:
            st.error("æ£€æµ‹å™¨æœªåŠ è½½")
        else:
            with st.spinner("æ­£åœ¨é‡ç®—..."):
                try:
                    recalc = active_detector.detect_single(new_text)
                    # ä»…æ›´æ–°ä¸´æ—¶ç»“æœï¼Œä¸ä¿®æ”¹ item
                    new_temp = item.copy()
                    new_temp.update({
                        "æ–‡æœ¬": new_text,
                        "AIç‡": recalc["ai_prob"],
                        "äººç±»ç‡": recalc["human_prob"],
                        "ç½®ä¿¡åº¦": recalc["confidence"],
                        "é¢„æµ‹": recalc["prediction"]
                    })
                    st.session_state[temp_result_key] = new_temp
                    # å¼ºåˆ¶åˆ·æ–°ä»¥æ›´æ–° metric_slot å’Œ text_area çš„ source
                    st.rerun()
                except Exception as exc:
                    st.error(f"é‡ç®—å¤±è´¥: {exc}")

    if col3.button("è®¡ç®—åˆ†å¸ƒ", key=f"calc_dist_{index}", use_container_width=True):
        if active_detector is None:
            st.error("æ£€æµ‹å™¨æœªåŠ è½½")
        elif not new_text.strip():
            st.error("å†…å®¹ä¸ºç©º")
        else:
            lang = st.session_state.get("language_code", "chinese")
            mode = "word"
            segments = segment_text(new_text, mode=mode, language=lang)
            sentences = split_into_sentences(new_text, language=lang)
            
            dist_results = analyze_contribution_systematic(active_detector, new_text, segments, sentences, language=lang)
            st.session_state[contrib_key] = dist_results
            st.rerun()

    if col4.button("é‡ç½®", key=f"reset_{index}", use_container_width=True):
        if "åŸæ–‡" in item:
            original = item["åŸæ–‡"]
            st.session_state.results[index]["æ–‡æœ¬"] = original
            st.session_state[reset_flag_key] = True
            if active_detector:
                recalc = active_detector.detect_single(original)
                st.session_state.results[index].update({
                    "AIç‡": recalc["ai_prob"],
                    "äººç±»ç‡": recalc["human_prob"],
                    "ç½®ä¿¡åº¦": recalc["confidence"],
                    "é¢„æµ‹": recalc["prediction"]
                })
            st.session_state.dialog_open = False
            st.session_state.editing_index = None
            if contrib_key in st.session_state:
                del st.session_state[contrib_key]
            st.rerun()


if dialog_decorator:
    @dialog_decorator("ç¼–è¾‘å†…å®¹")
    def show_edit_dialog(index):
        edit_form_content(index, st.session_state.get("detector"))


@st.cache_resource
def load_detector(language="chinese"):
    """åŠ è½½æ£€æµ‹å™¨ï¼ˆç¼“å­˜ï¼‰"""
    with st.spinner("æ­£åœ¨åŠ è½½æ¨¡å‹..."):
        detector = ChineseAIGCDetector(device="cpu", language=language)
    return detector


def split_into_sentences(text: str, language: str = "chinese") -> List[Tuple[str, int, int]]:
    """å°†æ–‡æœ¬åˆ†å‰²æˆå¥å­"""
    sentences = []
    if language == "chinese":
        pattern = r'[^ã€‚ï¼ï¼Ÿï¼›!?;]+[ã€‚ï¼ï¼Ÿï¼›!?;]*'
        for match in re.finditer(pattern, text):
            sentence = match.group().strip()
            if sentence:
                sentences.append((sentence, match.start(), match.end()))
    else:
        pattern = r'[^.!?]+[.!?]*'
        for match in re.finditer(pattern, text):
            sentence = match.group().strip()
            if sentence:
                sentences.append((sentence, match.start(), match.end()))
    if not sentences:
        sentences = [(text.strip(), 0, len(text))]
    return sentences

def segment_text(text: str, mode: str, language: str = "chinese") -> List[Tuple[str, int, int]]:
    """åˆ†è¯/åˆ†å­—"""
    segments = []
    if mode == "char":
        for i, char in enumerate(text):
            if char.strip():
                segments.append((char, i, i+1))
    else:
        if language == "chinese":
            words = jieba.tokenize(text)
            for word, start, end in words:
                if word.strip():
                    segments.append((word, start, end))
        else:
            pattern = r'\b\w+\b|[^\w\s]'
            for match in re.finditer(pattern, text):
                word = match.group()
                if word.strip():
                    segments.append((word, match.start(), match.end()))
    return segments

def analyze_contribution_systematic(detector: ChineseAIGCDetector, text: str, 
                                   segments: List[Tuple[str, int, int]],
                                   sentences: List[Tuple[str, int, int]],
                                   language: str = "chinese") -> List[Dict]:
    """ç³»ç»Ÿæ€§æ»‘åŠ¨çª—å£åˆ†æ"""
    original_result = detector.detect_single(text)
    original_ai_prob = original_result["ai_prob"]
    stats = defaultdict(lambda: {"present": [], "absent": []})
    
    # Progress UI placeholders
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    window_ratios = [1/4, 1/24]
    total_iterations = 0
    for sentence_text, sent_start, sent_end in sentences:
        sent_segments = [seg for seg in segments if seg[1] >= sent_start and seg[2] <= sent_end]
        if sent_segments:
            sent_len = len(sent_segments)
            for ratio in window_ratios:
                window_size = max(1, int(sent_len * ratio))
                num_positions = sent_len + window_size - 1
                total_iterations += num_positions
    
    current_iteration = 0
    
    for sent_idx, (sentence_text, sent_start, sent_end) in enumerate(sentences):
        sent_segment_info = []
        for global_idx, seg in enumerate(segments):
            if seg[1] >= sent_start and seg[2] <= sent_end:
                sent_segment_info.append((global_idx, seg))
        
        if not sent_segment_info:
            continue
        
        sent_segments_count = len(sent_segment_info)
        sent_result = detector.detect_single(sentence_text)
        sent_original_ai_prob = sent_result["ai_prob"]
    
        for ratio in window_ratios:
            window_size = max(1, int(sent_segments_count * ratio))
            for start_pos in range(-(window_size - 1), sent_segments_count):
                window_start = max(0, start_pos)
                window_end = min(sent_segments_count, start_pos + window_size)
                if window_start >= window_end:
                    continue
                
                deleted_local_indices = set(range(window_start, window_end))
                sent_text_parts = []
                for local_idx, (global_idx, seg) in enumerate(sent_segment_info):
                    if local_idx not in deleted_local_indices:
                        sent_text_parts.append(seg[0])
                
                modified_sent_text = ''.join(sent_text_parts)
                if modified_sent_text.strip():
                    try:
                        modified_result = detector.detect_single(modified_sent_text)
                        modified_ai_prob = modified_result["ai_prob"]
                    except:
                        modified_ai_prob = sent_original_ai_prob
                else:
                    modified_ai_prob = 0
                
                for local_idx, (global_idx, seg) in enumerate(sent_segment_info):
                    if local_idx in deleted_local_indices:
                        stats[global_idx]["absent"].append(modified_ai_prob)
                    else:
                        stats[global_idx]["present"].append(modified_ai_prob)
                
                current_iteration += 1
                if total_iterations > 0:
                    progress_bar.progress(current_iteration / total_iterations)

    progress_bar.empty()
    status_text.empty()
    
    results = []
    for idx, (segment, start, end) in enumerate(segments):
        present_probs = stats[idx]["present"]
        absent_probs = stats[idx]["absent"]
        
        if len(present_probs) > 5:
            present_sorted = sorted(present_probs)
            avg_present = np.mean(present_sorted[1:-1]) if len(present_sorted) > 2 else np.mean(present_probs)
        else:
            avg_present = np.mean(present_probs) if present_probs else original_ai_prob
            
        if len(absent_probs) > 5:
            absent_sorted = sorted(absent_probs)
            avg_absent = np.mean(absent_sorted[1:-1]) if len(absent_sorted) > 2 else np.mean(absent_probs)
        else:
            avg_absent = np.mean(absent_probs) if absent_probs else original_ai_prob
        
        contribution = avg_present - avg_absent
        results.append({
            "æ–‡æœ¬": segment,
            "èµ·å§‹ä½ç½®": start,
            "è´¡çŒ®åº¦": contribution,
            "å­˜åœ¨æ—¶AI": avg_present,
            "ç¼ºå¤±æ—¶AI": avg_absent
        })
    return results

def _lerp(a: int, b: int, t: float) -> int:
    return int(round(a + (b - a) * t))

def _hex_to_rgb(hex_color: str):
    hex_color = hex_color.lstrip("#")
    return int(hex_color[0:2], 16), int(hex_color[2:4], 16), int(hex_color[4:6], 16)

def _rgb_to_hex(rgb):
    return f"#{rgb[0]:02x}{rgb[1]:02x}{rgb[2]:02x}"

def get_contribution_color(contribution: float) -> str:
    normalized = (contribution + 0.1) / 0.2
    normalized = max(0.0, min(1.0, normalized))
    c0 = "#4caf50" 
    c1 = "#ff921e" 
    c2 = "#f23535" 
    if normalized <= 0.20: return c0
    elif normalized <= 0.50:
        t = (normalized - 0.20) / 0.30
        r0, g0, b0 = _hex_to_rgb(c0)
        r1, g1, b1 = _hex_to_rgb(c1)
        return _rgb_to_hex((_lerp(r0, r1, t), _lerp(g0, g1, t), _lerp(b0, b1, t)))
    elif normalized <= 0.60: return c1
    elif normalized <= 0.90:
        t = (normalized - 0.60) / 0.30
        r1, g1, b1 = _hex_to_rgb(c1)
        r2, g2, b2 = _hex_to_rgb(c2)
        return _rgb_to_hex((_lerp(r1, r2, t), _lerp(g1, g2, t), _lerp(b1, b2, t)))
    else: return c2

def generate_contribution_html(results: List[Dict]) -> str:
    sorted_results = sorted(results, key=lambda x: x["èµ·å§‹ä½ç½®"])

    # è®¡ç®—å½“å‰æ®µè½çš„æœ€å¤§æ­£å‘è´¡çŒ®å€¼ï¼Œç”¨äºç›¸å¯¹ç¼©æ”¾
    positive_contribs = [r["è´¡çŒ®åº¦"] for r in results if r.get("è´¡çŒ®åº¦", 0) > 0]
    max_contrib = max(positive_contribs) if positive_contribs else 0.001

    # è¿ç»­æ¸å˜ï¼šæ©™ -> çº¢ï¼ˆæŒ‰ç›¸å¯¹è´¡çŒ® ratio çº¿æ€§æ’å€¼ï¼‰
    c_low = "#ff921e"   # low highlight (orange)
    c_high = "#f23535"  # high highlight (red)

    def _ratio_to_color(ratio: float) -> str:
        ratio = max(0.0, min(1.0, ratio))
        r0, g0, b0 = _hex_to_rgb(c_low)
        r1, g1, b1 = _hex_to_rgb(c_high)
        return _rgb_to_hex((_lerp(r0, r1, ratio), _lerp(g0, g1, ratio), _lerp(b0, b1, ratio)))

    html_parts = []
    for result in sorted_results:
        segment = html.escape(result["æ–‡æœ¬"])
        contribution = float(result["è´¡çŒ®åº¦"])

        color = None

        # ä»…å¤„ç†æ­£å‘è´¡çŒ®ï¼ˆå¿½ç•¥è´Ÿå‘ï¼‰
        if contribution > 0:
            ratio = contribution / max_contrib if max_contrib > 0 else 0.0

            # ç»å¯¹é˜ˆå€¼ + ç›¸å¯¹é˜ˆå€¼ï¼šè¿‡æ»¤å™ªéŸ³ï¼Œä½†é¢œè‰²åœ¨é˜ˆå€¼ä»¥ä¸Šè¿ç»­æ¸å˜
            if contribution > 0.01 and ratio > 0.10:
                # å°† (0.10 ~ 1.0) æ˜ å°„åˆ° (0 ~ 1) åšè¿ç»­æ¸å˜
                t = (ratio - 0.10) / 0.90
                color = _ratio_to_color(t)

        tooltip = f"å­—/è¯: {segment} | è´¡çŒ®: {contribution*100:.2f}%"

        if color:
            html_parts.append(
                f'<span class="char-segment" style="border-bottom: 3px solid {color}; background-color: {color}25;" title="{tooltip}">{segment}</span>'
            )
        else:
            html_parts.append(
                f'<span class="char-segment" title="{tooltip}">{segment}</span>'
            )

    return ''.join(html_parts)


def extract_text_from_pdf(file) -> str:
    """
    ä» PDF æ–‡ä»¶æå–æ–‡æœ¬
    
    Args:
        file: ä¸Šä¼ çš„ PDF æ–‡ä»¶å¯¹è±¡
        
    Returns:
        æå–çš„æ–‡æœ¬å†…å®¹
    """
    try:
        pdf_reader = PyPDF2.PdfReader(file)
        text = ""
        for page in pdf_reader.pages:
            text += page.extract_text() + "\n"
        return text.strip()
    except Exception as e:
        st.error(f"PDF è§£æå¤±è´¥: {str(e)}")
        return ""


def extract_text_from_docx(file) -> str:
    """
    ä» Word æ–‡æ¡£æå–æ–‡æœ¬
    
    Args:
        file: ä¸Šä¼ çš„ Word æ–‡ä»¶å¯¹è±¡
        
    Returns:
        æå–çš„æ–‡æœ¬å†…å®¹
    """
    try:
        doc = Document(file)
        text = "\n".join([paragraph.text for paragraph in doc.paragraphs])
        return text.strip()
    except Exception as e:
        st.error(f"Word æ–‡æ¡£è§£æå¤±è´¥: {str(e)}")
        return ""


def extract_chinese_from_tex(file) -> str:
    """
    ä» LaTeX (.tex) æ–‡ä»¶ä¸­æå–æ­£æ–‡å†…å®¹
    åªä¿ç•™ document ç¯å¢ƒå†…çš„ä¸­æ–‡/è‹±æ–‡/æ•°å­—/æ ‡ç‚¹ä¸æ®µè½
    """
    try:
        content = file.read().decode("utf-8", errors="ignore")

        # 0) ä»…å– \begin{document} ... \end{document} å†…çš„æ­£æ–‡
        doc_match = re.search(r"\\begin\{document\}(.*)\\end\{document\}", content, flags=re.S)
        if doc_match:
            content = doc_match.group(1)

        # 1) å»æ³¨é‡Š
        content = re.sub(r"(?m)^%.*$", " ", content)

        # 2) å»\citeå¼•ç”¨
        content = re.sub(r"\\cite\{[^}]*\}", " ", content)

        # å¤„ç† itemize / enumerateï¼šæŠŠæ¯ä¸ª \item çš„å†…å®¹è¿æ¥æˆå•ç‹¬ä¸€æ®µï¼ˆæ®µå†…ä¸ä¿ç•™æ¢è¡Œï¼‰
        def _join_items(match):
            body = match.group(2)
            # æŒ‰ \item åˆ†å‰²å¹¶æ¸…ç†æ¯é¡¹
            parts = re.split(r"\\item", body)
            items = []
            for p in parts:
                p = p.strip()
                if not p:
                    continue
                # å»æ‰å†…éƒ¨å¤šä½™ç©ºç™½å’Œæ¢è¡Œï¼Œä¿ç•™å†…å®¹è¿ç»­æ€§
                p = re.sub(r"\s+", " ", p)
                items.append(p)
            # ç”¨ç©ºæ ¼è¿æ¥æ‰€æœ‰ itemï¼Œå½¢æˆä¸€æ®µ
            return " ".join(items)

        content = re.sub(r"\\begin\{(itemize|enumerate)\}(.*?)\\end\{\1\}", _join_items, content, flags=re.S)


        # 3) å»æ•°å­¦å…¬å¼ (è¡Œé—´/è¡Œå†…)
        math_patterns = [r"\$\$.*?\$\$", r"\\\[.*?\\\]", r"\\\(.*?\\\)", r"\$.*?\$"]
        for pat in math_patterns:
            content = re.sub(pat, " ", content, flags=re.S)

        # 4) å»ç¯å¢ƒå—
        content = re.sub(r"\\begin\{[^}]*\}.*?\\end\{[^}]*\}", " ", content, flags=re.S)

        # 5) å»å‘½ä»¤åä½†ä¿ç•™æ‹¬å·å†…å®¹
        content = re.sub(r"\\[a-zA-Z@]+(\s*\[[^\]]*\])?", " ", content)

        # 6) å»æ‰å¤§æ‹¬å·æœ¬èº«ï¼Œä¿ç•™å†…å®¹
        content = content.replace("{", "").replace("}", "")

        # 7) ä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—ã€å¸¸è§æ ‡ç‚¹ä¸æ¢è¡Œ/ç©ºç™½
        allowed = r"[^\u4e00-\u9fffA-Za-z0-9ã€‚ï¼Œï¼ï¼Ÿã€ï¼›ï¼šï¼šï¼ˆï¼‰ã€Šã€‹ã€ã€‘â€œâ€""''â€¦â€”\-\n\r\t ,.;:!\?\(\)\[\]\{\}/'\"`]"
        content = re.sub(allowed, " ", content)

        # 8) è§„èŒƒç©ºç™½ï¼Œä¿ç•™æ®µè½
        content = re.sub(r"[ \t]+", " ", content)
        content = re.sub(r"\n{3,}", "\n\n", content)

        return content.strip()
    except Exception as e:
        st.error(f"TeX è§£æå¤±è´¥: {str(e)}")
        return ""


def extract_text_from_md(file) -> str:
    """
    ä» Markdown (.md) æ–‡ä»¶ä¸­æå–æ–‡æœ¬å†…å®¹
    å»é™¤ Markdown æ ‡è®°ï¼Œä¿ç•™çº¯æ–‡æœ¬
    """
    try:
        content = file.read().decode("utf-8", errors="ignore")
        
        # å»é™¤ä»£ç å—
        content = re.sub(r"```.*?```", " ", content, flags=re.S)
        content = re.sub(r"`[^`]+`", " ", content)
        
        # å»é™¤å›¾ç‰‡é“¾æ¥
        content = re.sub(r"!\[([^\]]*)\]\([^\)]+\)", r"\1", content)
        
        # å»é™¤æ™®é€šé“¾æ¥ï¼Œä¿ç•™æ–‡æœ¬
        content = re.sub(r"\[([^\]]*)\]\([^\)]+\)", r"\1", content)
        
        # å»é™¤æ ‡é¢˜æ ‡è®°
        content = re.sub(r"^#+\s+", "", content, flags=re.M)
        
        # å»é™¤åˆ—è¡¨æ ‡è®°
        content = re.sub(r"^[\*\-\+]\s+", "", content, flags=re.M)
        content = re.sub(r"^\d+\.\s+", "", content, flags=re.M)
        
        # å»é™¤å¼•ç”¨æ ‡è®°
        content = re.sub(r"^>\s+", "", content, flags=re.M)
        
        # å»é™¤æ°´å¹³çº¿
        content = re.sub(r"^[\*\-_]{3,}$", "", content, flags=re.M)
        
        # å»é™¤ç²—ä½“å’Œæ–œä½“æ ‡è®°
        content = re.sub(r"\*\*([^\*]+)\*\*", r"\1", content)
        content = re.sub(r"__([^_]+)__", r"\1", content)
        content = re.sub(r"\*([^\*]+)\*", r"\1", content)
        content = re.sub(r"_([^_]+)_", r"\1", content)
        
        # è§„èŒƒç©ºç™½
        content = re.sub(r"[ \t]+", " ", content)
        content = re.sub(r"\n{3,}", "\n\n", content)
        
        return content.strip()
    except Exception as e:
        st.error(f"Markdown è§£æå¤±è´¥: {str(e)}")
        return ""


def split_into_paragraphs(text: str) -> List[str]:
    """
    å°†æ–‡æœ¬åˆ†å‰²ä¸ºæ®µè½
    
    Args:
        text: è¾“å…¥æ–‡æœ¬
        
    Returns:
        æ®µè½åˆ—è¡¨
    """
    # æŒ‰ç©ºè¡Œ
    paragraphs = re.split(r'[\n\n]', text.strip())
    # è¿‡æ»¤ç©ºæ®µè½
    paragraphs = [p.strip() for p in paragraphs if p.strip()]
    return paragraphs


def get_ai_rate_color(ai_prob: float) -> str:
    """æ ¹æ® AI æ¦‚ç‡è¿”å›å¯¹åº”çš„ CSS ç±»å"""
    if ai_prob > 0.75:
        return "ai-high"
    elif ai_prob > 0.5:
        return "ai-medium"
    else:
        return "ai-low"


def get_badge_class(ai_prob: float) -> str:
    """è·å–å¾½ç«  CSS ç±»å"""
    if ai_prob > 0.75:
        return "badge-high"
    elif ai_prob > 0.5:
        return "badge-medium"
    else:
        return "badge-low"


def render_ai_metric(slot, ai_prob: float):
    """åœ¨ç¼–è¾‘å¯¹è¯æ¡†ä¸­ä»¥çº¢é»„ç»¿å±•ç¤ºå½“å‰ AI ç‡"""
    percent = f"{ai_prob*100:.1f}%"
    if ai_prob > 0.75:
        color = "#f23535"
        icon = "ğŸ”´ é«˜åº¦ç–‘ä¼¼ AI"
    elif ai_prob > 0.5:
        color = "#ff921e"
        icon = "ğŸŸ¡ å¯èƒ½ AI"
    else:
        color = "#4caf50"
        icon = "ğŸŸ¢ å¯èƒ½äººç±»"
    slot.markdown(
        f"<div style='font-weight:600;font-size:18px;color:{color};'>å½“å‰ AI ç‡ï¼š{icon} | {percent}</div>",
        unsafe_allow_html=True,
    )


def format_ai_rate(ai_prob: float, human_prob: float) -> str:
    """æ ¼å¼åŒ– AI ç‡æ˜¾ç¤º"""
    ai_percent = f"{ai_prob*100:.1f}%"
    human_percent = f"{human_prob*100:.1f}%"
    
    # ç¡®å®šæ ‡ç­¾
    if ai_prob > 0.75:
        label = "ğŸ”´ é«˜åº¦ç–‘ä¼¼ AI"
    elif ai_prob > 0.5:
        label = "ğŸŸ¡ å¯èƒ½ AI"
    else:
        label = "ğŸŸ¢ å¯èƒ½äººç±»"
    
    return f"{label} | AI: {ai_percent} | äººç±»: {human_percent}"


def display_paragraph_result(para_num: int, paragraph: str, result: Dict, show_details: bool = False):
    """æ˜¾ç¤ºå•ä¸ªæ®µè½çš„æ£€æµ‹ç»“æœ - ç´§å‡‘æ ·å¼"""
    ai_prob = result["ai_prob"]
    human_prob = result["human_prob"]
    
    # ç¡®å®šé«˜äº®æ ·å¼
    if ai_prob > 0.75:
        highlight_class = "highlight-high"
        badge_class = "inline-badge-high"
        icon = "ğŸ”´"
    elif ai_prob > 0.5:
        highlight_class = "highlight-medium"
        badge_class = "inline-badge-medium"
        icon = "ğŸŸ¡"
    else:
        highlight_class = "highlight-low"
        badge_class = "inline-badge-low"
        icon = "ğŸŸ¢"
    
    # åˆ›å»ºç´§å‡‘çš„å†…è”æ˜¾ç¤º
    html_content = f"""
    <span class="text-segment {highlight_class}" title="AIç‡: {ai_prob*100:.1f}% | ç½®ä¿¡åº¦: {max(ai_prob, human_prob):.4f}">{paragraph}</span><span class="inline-badge {badge_class}">{icon}{ai_prob*100:.0f}%</span> 
    """
    
    st.markdown(html_content, unsafe_allow_html=True)


def main():
    """ä¸»å‡½æ•°"""
    
    # é¡¶éƒ¨æ ‡é¢˜
    st.title("AIGC æ–‡æœ¬æ£€æµ‹å™¨")
    st.markdown("### é€æ®µè½æ£€æµ‹ AI ç”Ÿæˆæ–‡æœ¬")
    
    # è¯­è¨€é€‰æ‹©å™¨
    language = st.radio(
        "é€‰æ‹©æ£€æµ‹è¯­è¨€ / Select Language",
        ("ğŸ‡¨ğŸ‡³ ä¸­æ–‡ (Chinese)", "ğŸ‡ºğŸ‡¸ è‹±æ–‡ (English)"),
        horizontal=True,
        help="ä¸­æ–‡æ¨¡å‹ï¼šyuchuantian/AIGC_detector_zhv3 | è‹±æ–‡æ¨¡å‹ï¼šyuchuantian/AIGC_detector_env3"
    )
    lang_code = "chinese" if "ä¸­æ–‡" in language else "english"
    
    # åŠ è½½æ£€æµ‹å™¨
    detector = load_detector(language=lang_code)
    st.session_state.detector = detector
    st.session_state.language_code = lang_code

        # é¢„ç•™æŸ¥è¯¢å‚æ•°å¤„ç†ï¼ˆå½“å‰æœªä½¿ç”¨ï¼‰
    
    
    
    
    
    
    
    # è¾“å…¥åŒºåŸŸ
    st.subheader("ğŸ“ è¾“å…¥æ–‡æœ¬")
    
    input_mode = st.radio(
        "é€‰æ‹©è¾“å…¥æ–¹å¼",
        ("ğŸ“„ ç›´æ¥è¾“å…¥æ–‡æœ¬", "ğŸ“ ä¸Šä¼ æ–‡ä»¶"),
        horizontal=True
    )
    
    text = ""
    
    if input_mode == "ğŸ“„ ç›´æ¥è¾“å…¥æ–‡æœ¬":
        placeholder_text = "åœ¨è¿™é‡Œç²˜è´´æˆ–è¾“å…¥æ‚¨è¦æ£€æµ‹çš„æ–‡æœ¬..." if lang_code == "chinese" else "Paste or type the text you want to detect here..."
        text = st.text_area(
            "è¯·è¾“å…¥è¦æ£€æµ‹çš„æ–‡æœ¬ (æ¯ä¸ªæ®µè½ä¼šå•ç‹¬æ£€æµ‹):",
            height=200,
            placeholder=placeholder_text,
            label_visibility="collapsed"
        )
    else:
        uploaded_file = st.file_uploader(
            "ä¸Šä¼ æ–‡ä»¶ (æ”¯æŒ txt, csv, pdf, docx, tex, md)",
            type=["txt", "csv", "pdf", "docx", "tex", "md"]
        )
        if uploaded_file:
            file_type = uploaded_file.name.split(".")[-1].lower()
            
            with st.spinner(f"æ­£åœ¨è§£æ {file_type.upper()} æ–‡ä»¶..."):
                if file_type == "pdf":
                    text = extract_text_from_pdf(uploaded_file)
                    if text:
                        st.success(f"âœ“ PDF è§£æå®Œæˆï¼Œæå– {len(text)} ä¸ªå­—ç¬¦")
                elif file_type in ["docx", "doc"]:
                    text = extract_text_from_docx(uploaded_file)
                    if text:
                        st.success(f"âœ“ Word æ–‡æ¡£è§£æå®Œæˆï¼Œæå– {len(text)} ä¸ªå­—ç¬¦")
                elif file_type == "tex":
                    text = extract_chinese_from_tex(uploaded_file)
                    if text:
                        st.success(f"âœ“ TeX å†…å®¹æå–å®Œæˆï¼Œæå– {len(text)} ä¸ªå­—ç¬¦")
                elif file_type == "md":
                    text = extract_text_from_md(uploaded_file)
                    if text:
                        st.success(f"âœ“ Markdown è§£æå®Œæˆï¼Œæå– {len(text)} ä¸ªå­—ç¬¦")
                elif file_type == "csv":
                    text = uploaded_file.read().decode("utf-8")
                else:  # txt
                    text = uploaded_file.read().decode("utf-8")
    
    # æ£€æµ‹æŒ‰é’®
    col1, col2, col3 = st.columns(3)
    
    if col1.button("ğŸ” å¼€å§‹æ£€æµ‹", use_container_width=False, type="primary"):
        
        if not text.strip():
            st.error("âŒ è¯·è¾“å…¥æ–‡æœ¬å†…å®¹")
        else:
            # åˆ†å‰²æ®µè½
            paragraphs = split_into_paragraphs(text)
            
            if not paragraphs:
                st.error("âŒ æ— æ³•è§£ææ–‡æœ¬")
            else:
                st.success(f"âœ“ å‘ç° {len(paragraphs)} ä¸ªæ®µè½ï¼Œæ­£åœ¨æ£€æµ‹...")
    
                # æ£€æµ‹æ‰€æœ‰æ®µè½
                progress_bar = st.progress(0)
                new_results = []
                
                for i, para in enumerate(paragraphs):
                    result = detector.detect_single(para)
                    new_results.append({
                        "æ®µè½": i + 1,
                        "æ–‡æœ¬": para,
                        "åŸæ–‡": para,
                        "AIç‡": result["ai_prob"],
                        "äººç±»ç‡": result["human_prob"],
                        "ç½®ä¿¡åº¦": result["confidence"],
                        "é¢„æµ‹": result["prediction"]
                    })
                    progress_bar.progress((i + 1) / len(paragraphs))
                
                st.session_state.results = new_results
                st.session_state.editing_index = None
                st.session_state.dialog_open = False

    # æ˜¾ç¤ºç»“æœ (å¦‚æœå­˜åœ¨)
    if "results" in st.session_state and st.session_state.results:
        results = st.session_state.results
        
        # æ˜¾ç¤ºå›¾è¡¨
        st.markdown("---")
        st.subheader("ç»Ÿè®¡")
        
        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        total_paragraphs = len(results)
        total_chars = sum(len(r["æ–‡æœ¬"]) for r in results)
        if total_chars == 0:
            total_chars = 1
        high_ai_count = sum(1 for r in results if r["AIç‡"] > 0.75)
        medium_and_high_count = sum(1 for r in results if r["AIç‡"] > 0.5)
        avg_ai_rate = sum(r["AIç‡"] * len(r["æ–‡æœ¬"]) for r in results) / total_chars
        avg_confidence = sum(r["ç½®ä¿¡åº¦"] for r in results) / total_paragraphs
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        col1, col2, col3, col4, col5 = st.columns(5)
        with col1:
            st.metric("æ€»æ®µè½æ•°", f"{total_paragraphs}")
        with col2:
            st.metric("ç–‘ä¼¼åŠä»¥ä¸Š", f"{medium_and_high_count}")
        with col3:
            st.metric("é«˜åº¦ç–‘ä¼¼", f"{high_ai_count}")
        with col4:
            st.metric("å¹³å‡ AI ç‡", f"{avg_ai_rate*100:.1f}%")
        with col5:
            st.metric("å¹³å‡ç½®ä¿¡åº¦", f"{avg_confidence:.3f}")
        
        st.markdown("<br>", unsafe_allow_html=True)
        
        # åˆ›å»ºä¸€ç»´æ¡å½¢å›¾ - æ‰€æœ‰æ®µè½åœ¨ä¸€è¡Œï¼Œå®½åº¦è¡¨ç¤ºå­—æ•°å æ¯”
        fig = go.Figure()
        
        # ä¸ºæ¯ä¸ªæ®µè½åˆ›å»ºä¸€ä¸ªå †å æ¡
        for i, r in enumerate(results):
            # é¢œè‰²æ ¹æ® AI ç‡ï¼ˆæ¸å˜ï¼šç»¿ -> æ©™ -> çº¢ï¼‰
            ai_rate = float(r["AIç‡"])

            def _lerp(a: int, b: int, t: float) -> int:
                return int(round(a + (b - a) * t))

            def _hex_to_rgb(hex_color: str):
                hex_color = hex_color.lstrip("#")
                return int(hex_color[0:2], 16), int(hex_color[2:4], 16), int(hex_color[4:6], 16)

            def _rgb_to_hex(rgb):
                return f"#{rgb[0]:02x}{rgb[1]:02x}{rgb[2]:02x}"

            def _ai_rate_to_color(p: float) -> str:
                p = max(0.0, min(1.0, p))
                c0 = "#4caf50"  # ä½ï¼šç»¿
                c1 = "#ff921e"  # ä¸­ï¼šæ©™
                c2 = "#f23535"  # é«˜ï¼šçº¢
                # åˆ†æ®µè§„åˆ™ï¼ˆæŒ‰ç™¾åˆ†æ¯”ï¼‰ï¼š
                # 0~20 çº¯ç»¿ï¼›20~50 å˜æ©™ï¼›50~60 çº¯æ©™ï¼›60~90 å˜çº¢ï¼›90~100 çº¯çº¢
                # 10~20 æœªæŒ‡å®šï¼Œé»˜è®¤ä¿æŒçº¯ç»¿ï¼ˆä¸ 0~10 ä¸€è‡´ï¼‰
                if p <= 0.20:
                    return c0
                if p <= 0.50:
                    t = (p - 0.20) / 0.30
                    r0, g0, b0 = _hex_to_rgb(c0)
                    r1, g1, b1 = _hex_to_rgb(c1)
                    return _rgb_to_hex((_lerp(r0, r1, t), _lerp(g0, g1, t), _lerp(b0, b1, t)))
                if p <= 0.60:
                    return c1
                if p <= 0.90:
                    t = (p - 0.60) / 0.30
                    r1, g1, b1 = _hex_to_rgb(c1)
                    r2, g2, b2 = _hex_to_rgb(c2)
                    return _rgb_to_hex((_lerp(r1, r2, t), _lerp(g1, g2, t), _lerp(b1, b2, t)))
                return c2

            color = _ai_rate_to_color(ai_rate)
            
            # å®½åº¦æ ¹æ®å­—æ•°å æ¯”
            width = len(r["æ–‡æœ¬"]) / total_chars * 100
            
            fig.add_trace(go.Bar(
                name=f"æ®µè½ {r['æ®µè½']}",
                x=[width],
                y=["å…¨æ–‡"],
                orientation='h',
                marker=dict(
                    color=color,
                    line=dict(width=0)
                ),
                hovertemplate=f"<b>æ®µè½ {r['æ®µè½']}</b><br>å­—æ•°: {len(r['æ–‡æœ¬'])}<br>AIç‡: {r['AIç‡']*100:.1f}%<extra></extra>"
            ))
        
        fig.update_layout(
            title="æ¦‚è§ˆ",
            xaxis=dict(
                title="",
                showticklabels=False,
                showgrid=False
            ),
            yaxis=dict(
                showticklabels=False,
                showgrid=False
            ),
            barmode='stack',
            height=150,
            showlegend=False,
            margin=dict(l=0, r=0, t=40, b=20)
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
    
        # æ®µè½è§†å›¾
        st.markdown("---")
        st.subheader("ğŸ“„ æ®µè½è§†å›¾")

        edit_mode_enabled = st.session_state.get("edit_mode_enabled", False)
        toggle_col = st.columns(1)
        with toggle_col[0]:
            if not edit_mode_enabled:
                if st.button("å¯ç”¨ç¼–è¾‘", use_container_width=True):
                    st.session_state.edit_mode_enabled = True
                    st.rerun()
            else:
                if st.button("å…³é—­ç¼–è¾‘", use_container_width=True):
                    st.session_state.edit_mode_enabled = False
                    st.session_state.editing_index = None
                    st.session_state.dialog_open = False
                    st.rerun()
 
        # å¦‚æœå¯¹è¯æ¡†éœ€è¦ä¿æŒæ‰“å¼€ï¼Œæ ¹æ®çŠ¶æ€é‡æ–°å±•ç¤º
        if edit_mode_enabled and dialog_decorator and st.session_state.get("dialog_open") and st.session_state.get("editing_index") is not None:
            show_edit_dialog(st.session_state.editing_index)
        
        # Fallback edit area if no dialog support
        if edit_mode_enabled and not dialog_decorator and "editing_index" in st.session_state:
            idx = st.session_state.editing_index
            if idx is not None and 0 <= idx < len(results):
                with st.container():
                    st.info(f"æ­£åœ¨ç¼–è¾‘æ®µè½ {idx+1}")
                    edit_form_content(idx, st.session_state.get("detector"))
                    if st.button("å…³é—­ç¼–è¾‘", key="close_edit"):
                        del st.session_state.editing_index
                        st.rerun()

        # æ˜¾ç¤ºæ‰€æœ‰æ®µè½ï¼Œå°¾éƒ¨å°ç¬”æŒ‰é’®è§¦å‘ç¼–è¾‘
        for i, result in enumerate(results):
            with st.container():
                col_text, col_btn = st.columns([0.985, 0.05])
                with col_text:
                    display_paragraph_result(result["æ®µè½"], result["æ–‡æœ¬"], {
                        "ai_prob": result["AIç‡"],
                        "human_prob": result["äººç±»ç‡"],
                        "confidence": result["ç½®ä¿¡åº¦"]
                    })
                with col_btn:
                    if edit_mode_enabled and st.button("âœ", key=f"btn_edit_{i} edit-btn-small", help="ç¼–è¾‘æ­¤æ®µè½", use_container_width=True):
                        st.session_state.editing_index = i
                        if dialog_decorator:
                            st.session_state.dialog_open = True
                        st.rerun()
        
        # æ˜¾ç¤ºå¯¼å‡ºæŒ‰é’®
        st.markdown("---")
                
        # å¯¼å‡ºä¸º CSV
        csv_data = pd.DataFrame(results)
        csv_data["AIç‡"] = csv_data["AIç‡"].apply(lambda x: f"{x*100:.1f}%")
        csv_data["äººç±»ç‡"] = csv_data["äººç±»ç‡"].apply(lambda x: f"{x*100:.1f}%")
        
        csv = csv_data.to_csv(index=False)
        col1, col2 = st.columns(2)
        with col1:
            st.download_button(
                label="ğŸ“¥ ä¸‹è½½æ£€æµ‹ç»“æœ (CSV)",
                data=csv,
                file_name="detection_results.csv",
                mime="text/csv",
                use_container_width=True
            )
            
        with col2:
            # ä¸‹è½½å·²ç¼–è¾‘åçš„æ•´ç¯‡æ–‡æœ¬
            edited_full_text = "\n\n".join(r["æ–‡æœ¬"] for r in results)
            st.download_button(
                label="ğŸ“„ ä¸‹è½½å·²ç¼–è¾‘æ–‡æœ¬",
                data=edited_full_text,
                file_name="edited_text.txt",
                mime="text/plain",
                use_container_width=True
            )
    
    
    # é¡µè„šä¿¡æ¯
    st.markdown("---")
    model_display = "yuchuantian/AIGC_detector_zhv3" if lang_code == "chinese" else "yuchuantian/AIGC_detector_env3"
    st.markdown(f"""
    <div style='text-align: center; color: #888; font-size: 12px;'>
        <p>
        AIGC æ£€æµ‹å™¨ v3.0 | 
        å½“å‰æ¨¡å‹: {model_display}
        </p>
        <p>
        âš ï¸ ä»…ä¾›å­¦æœ¯ç ”ç©¶ä½¿ç”¨
        </p>
    </div>
    """, unsafe_allow_html=True)


if __name__ == "__main__":
    main()


